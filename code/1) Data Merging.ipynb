{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script merges consumption, tariff, and weather data, and performs necessary data cleaning and aggregation.\n",
    "\n",
    "Data sources:\n",
    "- Dynamic Tariff Data: 'tariff_d.csv'\n",
    "- Consumption Data for Group D: 'consumption_d.csv'\n",
    "- London Weather Data for 2013: csv files that start with 'London,UK'\n",
    "\n",
    "Note: Ensure all data files are in the same directory as this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process consumption and tariff data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tariff_data = pd.read_csv('tariff_d.csv', parse_dates=['GMT'])\n",
    "consumption_data = pd.read_csv('consumption_d.csv', parse_dates=['GMT'])\n",
    "\n",
    "# Filter data for 2013\n",
    "tariff_data_filtered = tariff_data[tariff_data['GMT'].dt.year == 2013].copy()\n",
    "consumption_data_filtered = consumption_data[\n",
    "    (consumption_data['GMT'].dt.year == 2013) & \n",
    "    (consumption_data['GMT'] != '2013-01-01 00:00:00')\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert consumption data to long format and handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of households removed due to >5% missing values: 18\n",
      "Number of unique remaining household_ids: 1007\n"
     ]
    }
   ],
   "source": [
    "consumption_melted = consumption_data_filtered.melt(id_vars=['GMT'], var_name='household_id', value_name='consumption')\n",
    "all_households = pd.DataFrame(index=consumption_melted.groupby('household_id').size().index)\n",
    "all_households['missing_values'] = consumption_melted[consumption_melted['consumption'].isnull()].groupby('household_id').size().reindex(all_households.index, fill_value=0)\n",
    "all_households['percentage_missing'] = (all_households['missing_values'] / consumption_melted.groupby('household_id').size()) * 100\n",
    "households_to_remove = all_households[all_households['percentage_missing'] > 5].index\n",
    "consumption_melted_filtered = consumption_melted[~consumption_melted['household_id'].isin(households_to_remove)]\n",
    "\n",
    "print(f\"Number of households removed due to >5% missing values: {len(households_to_remove)}\")\n",
    "print(f\"Number of unique remaining household_ids: {consumption_melted_filtered['household_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in consumption_imputed after imputation:\n",
      "GMT             0\n",
      "household_id    0\n",
      "consumption     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "consumption_imputed = consumption_melted_filtered.copy()\n",
    "consumption_imputed = consumption_imputed.set_index('GMT')\n",
    "consumption_imputed['consumption'] = consumption_imputed.groupby('household_id')['consumption'].transform(lambda x: x.interpolate(method='time').ffill().bfill())\n",
    "consumption_imputed = consumption_imputed.reset_index()\n",
    "\n",
    "# Check for missing values after imputation\n",
    "missing_values_after_imputation = consumption_imputed.isnull().sum()\n",
    "print(\"Missing values in consumption_imputed after imputation:\")\n",
    "print(missing_values_after_imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data to hourly granularity and merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption_imputed['hour'] = (consumption_imputed['GMT'] + pd.Timedelta(minutes=30)).dt.floor('H')\n",
    "tariff_data_filtered['hour'] = (tariff_data_filtered['GMT'] + pd.Timedelta(minutes=30)).dt.floor('H')\n",
    "consumption_hourly = consumption_imputed.groupby(['hour', 'household_id'])['consumption'].sum().reset_index()\n",
    "tariff_hourly = tariff_data_filtered.groupby('hour')['Price'].mean().reset_index()\n",
    "merged_data = pd.merge(consumption_hourly, tariff_hourly, on='hour', how='left')\n",
    "merged_data.rename(columns={'hour': 'timestamp', 'Price': 'tariff'}, inplace=True)\n",
    "merged_data.sort_values(by=['timestamp', 'household_id'], inplace=True)\n",
    "merged_data.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumption and tariff data processed and saved.\n",
      "Shape of merged data: (8821320, 3)\n",
      "Columns in merged data: ['household_id', 'consumption', 'tariff']\n"
     ]
    }
   ],
   "source": [
    "# Save merged consumption and tariff data\n",
    "merged_data.to_csv('1a) consumption_and_tariff_data_hourly.csv')\n",
    "\n",
    "print(\"Consumption and tariff data processed and saved.\")\n",
    "print(f\"Shape of merged data: {merged_data.shape}\")\n",
    "print(f\"Columns in merged data: {merged_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the weather CSV files\n",
    "csv_files = glob.glob('London,UK*.csv')\n",
    "\n",
    "def read_csv_with_datetime_index(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime']) \n",
    "    df.set_index('datetime', inplace=True)\n",
    "    return df\n",
    "\n",
    "# Read and concatenate all CSV files, keep important columns\n",
    "df_list = [read_csv_with_datetime_index(file) for file in csv_files]\n",
    "merged_df = pd.concat(df_list)\n",
    "merged_df.sort_index(inplace=True)\n",
    "merged_df = merged_df[['solarradiation', 'windspeed', 'temp', 'precip', 'humidity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicate timestamps and handle daylight savings time change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated missing hour at: 2013-03-31 01:00:00\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with duplicate timestamps, keeping only the first occurrence\n",
    "merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n",
    "time_diffs = merged_df.index.to_series().diff().dropna()\n",
    "unusual_entries = time_diffs[time_diffs != pd.Timedelta(hours=1)]\n",
    "\n",
    "for timestamp, time_diff in unusual_entries.items():\n",
    "    if time_diff == pd.Timedelta(hours=2):\n",
    "        prev_timestamp = timestamp - pd.Timedelta(hours=2)\n",
    "        next_timestamp = timestamp\n",
    "        interpolated_row = merged_df.loc[prev_timestamp].combine(merged_df.loc[next_timestamp], lambda x, y: (float(x) + float(y)) / 2)\n",
    "        interpolated_timestamp = prev_timestamp + pd.Timedelta(hours=1)\n",
    "        merged_df.loc[interpolated_timestamp] = interpolated_row\n",
    "        print(f\"Interpolated missing hour at: {interpolated_timestamp}\")\n",
    "\n",
    "# Sort the DataFrame again to ensure the interpolated row is in the correct position\n",
    "merged_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data processed and saved.\n",
      "Shape of weather data: (8760, 5)\n",
      "Columns in weather data: ['solarradiation', 'windspeed', 'temp', 'precip', 'humidity']\n",
      "\n",
      "Data preprocessing complete. Output files: '1a) consumption_and_tariff_data_hourly.csv' and '1b) merged_weather_data.csv'\n"
     ]
    }
   ],
   "source": [
    "merged_df.to_csv('1b) merged_weather_data.csv')\n",
    "\n",
    "print(\"Weather data processed and saved.\")\n",
    "print(f\"Shape of weather data: {merged_df.shape}\")\n",
    "print(f\"Columns in weather data: {merged_df.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nData preprocessing complete. Output files: '1a) consumption_and_tariff_data_hourly.csv' and '1b) merged_weather_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
